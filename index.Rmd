---
title: "Computational Musicology"
author: "Lieve"
date: "2/23/2022"
output:
  flexdashboard::flex_dashboard:
    orientation: columns
    storyboard: true
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(flexdashboard)
library(tidyverse)
library(plotly)
library(spotifyr)
library(ggplot2)
library(compmus)
library(devtools)
library(tidymodels)
library(heatmaply)
library(protoclust)
library(ggpubr)
```
# Introduction

## Column 1 {data-width = 650}

### Corpus

Sitting in a cafe or restaurant, music has a big influence on the atmosphere. Working in a cafe myself, I use a lot of spotify playlists. In this cafe we have different playlists for different moments of the day. For my corpus in this research I chose our lunchtime and after dinnertime playlists. I want to find out what aspects of this music makes them more enjoyable on different parts of the day. For this I will compare these two playlists with each other. Beforehand, I would expect differences mostly in the tempo and energy of the tracks. Looking at valence, I would expect more neutral or a bit more negative songs for the lunch playlists. For the evening playlist I would expect the songs to be a bit more positive, because during this time people are drinking more alcohol and maybe going to a party afterwards.

- beschrijven van verschillen tussen playists. En verwachte verschillen.

To investigate the differences I make use of a variation of visualisations and spotify features. First I will visualize track-level features for songs in both playlists. Secondly I will analyse the chroma features of both playlists by using chromagrams. 


## Column 2 {data-width = 350}

### Lunch

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/15XTzdCk80I6KrHwrRgR0L?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

### Dinner

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/7fx9x4iegCA43TmBIKdvGn?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>


# Analysis {.storyboard}

### Comparing energy, valence and loudness

```{r}
lunch <- get_playlist_audio_features("","15XTzdCk80I6KrHwrRgR0L")
dinner <- get_playlist_audio_features("", "7fLp6h0KxTCnNWsbBpSWXi")
bar_playlists <-
  bind_rows(
    lunch %>% mutate(category = "Lunch"),
    dinner %>% mutate(category = "Dinner"))

plot_1 <- bar_playlists %>% mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
  ) %>%ggplot(                     
    aes(
      x = valence,
      y = energy,
      size = loudness,
      colour = mode
    )
  ) +
  geom_point() +              
  geom_rug(size = 0.1) +      
  geom_text(                  
    aes(
      x = valence,
      y = energy,
      label = label
    ),
    data = 
      tibble(
        label = c("", ""),
        category = c("Dinner", "Lunch"),
        valence = c(0.090, 0.123),
        energy = c(0.101, 0.967)
      ),
    colour = "black",        
    size = 3,                
    hjust = "left",          
    vjust = "bottom",        
    nudge_x = -0.05,          
    nudge_y = 0.02            
  ) +
  facet_wrap(~category) +     
  scale_x_continuous(         
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),   
    minor_breaks = NULL       
  ) +
  scale_y_continuous(         
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_colour_brewer(        
    type = "qual",            
    palette = "Paired"        
  ) +
  scale_size_continuous(      
    trans = "exp",            
    guide = "none"            
  ) +
  theme_light() +             
  labs(                       
    x = "Valence",
    y = "Energy",
    colour = "Mode"
  )

ggplotly(plot_1)
```

***

For the first plot, I put valence and energy on the axes. The size of the dots represent the loudness of the tracks. In the plot you can see that the tracks of the evening playlist are predominantly higher in energy, as I expected. The valence is also higher, so closer to the value 1. This means that the track are more positively in their nature. The lunch playlist is very linearly divided. In this playlist it seems that there is a connection between valence and energy. When the valence is higher, so the track is more positive, the energy of the track is also higher. 

Also in the size of the dots there is a big difference. The dots of the lunch playlist are almost all relatively small, where the dots of the evening playlists are more divided. But, you can see that the loudness in the evening playlist seems to be higher. These three track based features show that the evening playlist scores higher in valence, energy and loudness. These were results I expected, because they fit in with the atmossphere created in a restaurants at these times of day. During lunch it is nice to have calmer, more quiet atmossphere. In the evening people like to be more energised and ready for the night ahead. For the next part I want to investigate chroma and timbre based features.

### Evaluating self-similarity for chroma and timbre features

```{r lunch}
bzt_lunch <-
  get_tidy_audio_analysis("4Hhv2vrOTy89HFRcjU3QOx") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
chroma_lunch <- bind_rows(
  bzt_lunch %>% 
    compmus_self_similarity(pitches, "aitchison") %>% 
    mutate(d = d / max(d), type = "Chroma"),
  bzt_lunch %>% 
    compmus_self_similarity(timbre, "euclidean") %>% 
    mutate(d = d / max(d), type = "Timbre")
) %>%
  mutate() %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "")

bzt_dinner <-
  get_tidy_audio_analysis("0lO9vvpJdd2ZNAq61ga9wy") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
chroma_dinner <- bind_rows(
  bzt_dinner %>% 
    compmus_self_similarity(pitches, "aitchison") %>% 
    mutate(d = d / max(d), type = "Chroma"),
  bzt_dinner %>% 
    compmus_self_similarity(timbre, "euclidean") %>% 
    mutate(d = d / max(d), type = "Timbre")
) %>%
  mutate() %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "")

ggarrange(chroma_lunch, chroma_dinner, 
          labels = c("Lunch", "Evening"),
          ncol = 1, nrow = 2)
```

***

In these plots you can see chroma and timbre based features for a song of both playlists. I wanted to pick a random track, because the research is about the playlists in general. So all tracks are a representation of the atmosphere created by the playlist. I chose for both playlists the 5th track of the playlist. For the lunch playlist this is 'At last' from Etta James, for the evening playlist this is 'Doom and Gloom' from The Rolling Stones.

For 'At last' the chroma based matrix does not show distinct changes. The timbre based matrix shows one distinct moment at 140-150. Listening to the song you can hear that the piano is very repetitive during the song. At this moment however, the piano uses a different melody. For 'Doom and Gloom' the chroma based matrix again is hard to read for distinct changes. However, the timbre based matrix shows several clear moments where the music changes. This is at 60 and 150. At 60, there is a short guitar solo. At 150, after a period of instrumental music the singing starts again, but all instruments except the drums stop playing. 

Altough these matrices may not show a lot of difference between the playlists, the instruments used do seem distinctive to me. The piano suits better with calmer, softer music as we saw in the lunch playlist on the previous page. The drums suit better with energetic, louder music like we saw in the evening playlist. However, all instruments fit with different tempos. That is why in the next part I am looking at the tempo of the playlists.

### Difference in tempo

```{r}
lunch <- get_playlist_audio_features("","15XTzdCk80I6KrHwrRgR0L")

lunch_tempo <- ggplot(data = lunch, mapping = aes(x = tempo)) +
    geom_histogram(aes(fill = mode)) +
    facet_wrap(~ mode)

dinner <- get_playlist_audio_features("","7fx9x4iegCA43TmBIKdvGn")

dinner_tempo <- ggplot(data = dinner, mapping = aes(x = tempo)) +
    geom_histogram(aes(fill = mode)) +
    facet_wrap(~ mode)

ggarrange(lunch_tempo, dinner_tempo, 
          labels = c("Lunch", "Dinner"),
          ncol = 2, nrow = 1)
```

***

Explanation

You can see that the tempo bla bla bla

### Keygrams

```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}
major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
keygram_lunch <- get_tidy_audio_analysis("56IQUfsUskt9VHe6YM6kue") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) %>%
  compmus_match_pitch_template(key_templates, "aitchison", "manhattan") %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "", fill = "Distance")

circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}
major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
keygram_dinner <- get_tidy_audio_analysis("45Ia1U4KtIjAPPU7Wv1Sea") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) %>%
  compmus_match_pitch_template(key_templates, "aitchison", "manhattan") %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "", fill = "Distance")

ggarrange(keygram_lunch, keygram_dinner, 
          labels = c("Lunch", "Dinner"),
          ncol = 1, nrow = 2)

```

***

For these visualizations I chose two songs that are very representative for the playlists. 

- which ones and why?

I think you see very good that in the evening playlist, there is a lot more variation in the keys used. This could be explained by the lunch playlist being more calm and equal, so also the difference in keys being more equal.

### Dendogram for the lunch playlist

```{r olifant}
olifant <- 
    get_playlist_audio_features('bnfcollection', '7fx9x4iegCA43TmBIKdvGn') %>% 
    add_audio_analysis %>% 
    mutate(
        segments = 
            map2(segments, key, compmus_c_transpose)) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'),
        timbre =
            map(
                segments,
                compmus_summarise, timbre,
                method = 'mean')) %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'clr')) %>% 
    mutate_at(vars(pitches, timbre), map, bind_rows) %>% 
    unnest(cols = c(pitches, timbre))
olifant_juice <- 
    recipe(track.name ~
               danceability +
               energy +
               loudness +
               speechiness +
               acousticness +
               instrumentalness +
               liveness +
               valence +
               tempo +
               duration +
               C + `C#|Db` + D + `D#|Eb` +
               E + `F` + `F#|Gb` + G +
               `G#|Ab` + A + `A#|Bb` + B +
               c01 + c02 + c03 + c04 + c05 + c06 +
               c07 + c08 + c09 + c10 + c11 + c12,
           data = olifant) %>% 
    step_range(all_predictors()) %>% 
    prep(olifant %>% mutate(track.name = str_trunc(track.name, 20))) %>% 
    juice %>% 
    column_to_rownames('track.name')
ggheatmap(
    olifant_juice,
    hclustfun = protoclust,
    dist_method = 'manhattan'
)
```

***

Explanation

This dendogram is made with the lunch playlist. In the dendogram you can see that there are several clusters to be found. However, due to the large amount of data it is not very readable yet. I am thinking of reducing the number of songs used. Maybe by creating a playlist with songs from both lunch as evening playlists. In this way, I could maybe use the dendogram to see if based on features, the songs from the same playlists are clustered together.

# Conclusion


